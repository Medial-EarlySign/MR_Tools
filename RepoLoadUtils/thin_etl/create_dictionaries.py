from Configuration import Configuration
from common import *
import os, re, io
from datetime import datetime
from prep_med import *
import therapy

def get_first_line(fp):
    f = open(fp, 'r')
    line = f.readline().strip()
    f.close()
    return line
    
def buildAhdLookup(maxGroupSize = 1000):
    cfg  = Configuration()
    work_dir = fixOS(cfg.work_path)
    doc_path = fixOS(cfg.doc_source_path)
    if not(os.path.exists(work_dir)):
        raise NameError('config error - work dir wasn''t found')
    if not(os.path.exists(doc_path)):
        raise NameError('config error - doc dir wasn''t found')
    ahd_lookup_name = list(filter(lambda f: f.startswith('AHDlookup') ,os.listdir(doc_path)))
    if len(ahd_lookup_name) != 1:
        raise NameError('coudn''t find AHDlookup')
    ahd_lookup_name = ahd_lookup_name[0]
    ahd_path = os.path.join(doc_path, ahd_lookup_name)
    if not(os.path.exists(ahd_path)):
        raise NameError('ahd_lookup file wan''t found %s'%ahd_path)

    readCodesignals = ['Cervical', 'Family_History', 'Scoring_test_result', 'Allergy_over_intolerance']
    basePath = os.path.join(work_dir, 'SpecialSignals')
    allFiles = os.listdir(basePath)
    allFiles = list(filter(lambda x: not(x in readCodesignals) ,allFiles))
    sig_names = list(map(lambda f: get_first_line(os.path.join(basePath,f)).split('\t')[1] ,allFiles ))

    fp = open(ahd_path)
    lines = fp.readlines()
    fp.close()
    lines = list(filter(lambda line: len(line.strip()) > 0 ,lines))
    codes = list(map(lambda line: [line[:8].strip(), line[38:44].strip(), line[44:].strip()] ,lines))
    tuples = dict()
    known_keys = set()
    for baseCode,code, val in codes:
        if len(val) == 0:
            print('empty in %s'%code)
            val = '<None>'
        grps = [code, val]
        known_keys.add(code)
        if tuples.get(baseCode) is None:
            tuples[baseCode] = []
        tuples[baseCode].append(grps)
    known_keys.add('EMPTY_THIN_FIELD')

    #cat /server/Temp/Thin_2017_Loading/repository_data/../clinical/FECAL_TEST | awk 'BEGIN {FS="\t"} {d[$4]+=1} END {for (i in d) {print i,d[i]}}' | sort -n -k2 -r
    fp = open(os.path.join(work_dir, 'clinical', 'Colonscopy'))
    lines = fp.readlines()
    fp.close()
    uniq_keys = set(map(lambda ln : ln.split('\t')[3].strip() ,lines))
    fp = open(os.path.join(work_dir, 'clinical', 'FECAL_TEST'))
    lines = fp.readlines()
    uniq_keys.update(map(lambda ln : ln.split('\t')[3].strip() ,lines))
    fp.close()
    key_ls = list(uniq_keys)
    add_keys = list(filter(lambda k: k not in known_keys,key_ls))
    tuples['FECAL_AND_COLONOSCOPY_UNKNOWN_FLAGS'] = []
    for key in add_keys:
        tuples['FECAL_AND_COLONOSCOPY_UNKNOWN_FLAGS'].append([key, 'UNKNOWN'])
    tuples['FECAL_AND_COLONOSCOPY_UNKNOWN_FLAGS'] = sorted(tuples['FECAL_AND_COLONOSCOPY_UNKNOWN_FLAGS'], key = lambda k : k[0])
    #['@PA'], ['@H'], ['@L'], ['@A'], ['Return to the'] , ['Report Scope']  #create by script automatically for FECAL_TEST and Colonscopy

    config_signals = os.path.join(work_dir, 'rep_configs', 'codes_to_signals')
    if not(os.path.exists(config_signals)):
        raise NameError('Missing codes_to_signals. looked for it here:%s'%config_signals)
    fp = open(config_signals)
    lines = fp.readlines()
    fp.close()
    lines = list(filter(lambda ln : len(ln.strip()) > 0 and not(ln.startswith('#')),lines))
    convert_names = dict()
    for line in lines:
        tokens = line.split('\t')
        convert_names[tokens[0].strip()] = tokens[1].strip()

    sig_names = list(map(lambda nm: nm if convert_names.get(nm) is None else convert_names[nm] ,sig_names))
    headerLine = '# This File Was Auto Generated By create_dictionaries.py on %s file @ %s\n'%(ahd_lookup_name, datetime.now().strftime('%d-%m-%Y %H:%M:%S'))
    sectionLine = 'SECTION\tAHDlookups,Colonscopy,FECAL_TEST,%s\n'%(','.join(sig_names))
    toWrite = [sectionLine, headerLine, '\n', 'DEF\t0\tEMPTY_THIN_FIELD\n', '\n']

    ordered = []
    for baseCode, tups in tuples.items():
        ordered.append([baseCode, tups])
    ordered = sorted(ordered, key = lambda t: t[0])
    #return tuples
    startCode = maxGroupSize
    for baseCode, tups in ordered:
        toWrite.append('#%s codes:\n'%baseCode)
        codeLen = []
        if len(tups) > maxGroupSize:
            raise NameError('%d resulotion is not enougth - got %d elements'%(maxGroupSize,len(tups)))
        for tp in tups:
            line = 'DEF\t%d\t%s\n'%( startCode ,tp[0] )
            toWrite.append(line)
            if len(tp) > 1: #for the unkown value skip definition
                line = 'DEF\t%d\t%s\n'%( startCode ,tp[1].replace(' ','_') )
                toWrite.append(line)
            startCode += 1
            codeLen.append(len(tp[0]))
        startCode = maxGroupSize*(int(startCode/maxGroupSize)+1) #move to next code zone
        toWrite.append('\n')

    dict_path = os.path.join(work_dir, 'dicts')
    if not(os.path.exists(dict_path)):
        os.makedirs(dict_path)
    fw = io.open(os.path.join(dict_path, 'dict.ahdlookups'), 'w', newline = '')
    fw.writelines(list(map(lambda ln : unicode(ln) ,toWrite)))
    fw.close()
    return tuples

def create_pvi_dict():
    cfg = Configuration()
    data_path = fixOS(cfg.raw_source_path)
    work_dir = fixOS(cfg.work_path)
    in_file_names = list(filter(lambda f: f.startswith('pvi.') ,os.listdir(data_path)))
    dict_path = os.path.join(work_dir, 'dicts')
    if not(os.path.exists(dict_path)):
        os.makedirs(dict_path)
    all_lines = set()
    for in_filename in in_file_names:
        full_path = os.path.join(data_path, in_filename)
        fp = open(full_path, 'r')
        lines = fp.readlines()
        fp.close()
        lines = list(map(lambda ln : ln[4:16] ,lines))
        all_lines.update(lines)
    all_lines = sorted(all_lines)

    fw = io.open(os.path.join(dict_path, 'dict.pvi'), 'w', newline ='')
    fw.write(unicode('SECTION	PVI\n'))
    for i in range(len(all_lines)):
        fw.write(unicode('DEF\t%d\tPVI_%s\n'%(i+1, all_lines[i])))
    fw.close()

    fw = io.open(os.path.join(dict_path, 'distinct_pvi_flags_set'), 'w', newline ='')
    for i in range(len(all_lines)):
        fw.write(unicode('SET\tPVI\tPVI_%s\n'%(all_lines[i])))
    fw.close()

def create_drugs_nice_names():
    import pandas as pd
    cfg = Configuration()
    work_dir = fixOS(cfg.work_path)
    dict_path = os.path.join(work_dir, 'dicts')
    if not(os.path.exists(dict_path)):
        os.makedirs(dict_path)
    d = pd.read_csv(os.path.join(dict_path,'dict.drugs_defs'), sep='\t', names = ['d', 'drug_id', 'drug_thin_id'])
    s = pd.read_csv(os.path.join(dict_path,'dict.drugs_sets'), sep='\t', names = ['s', 'set_id', 'drug_thin_id'])
    #print (d.shape, s.shape)

    drugs = d[d.drug_thin_id.apply(lambda x: str(x)[:2] == "dc")]
    filt = drugs.drug_thin_id.apply(lambda x: str(x)[2] != ':')
    #drugs.loc[:, 'short_name'] = drugs.drug_thin_id.apply(lambda x: str(x)[2] != ':')
    drugs_ids = drugs[filt][['drug_id', 'drug_thin_id']]
    drugs_desc = drugs[~filt][['drug_id', 'drug_thin_id']]
    drugs_desc.columns = ['drug_id', 'drug_desc']
    drugs = pd.merge(drugs_ids, drugs_desc, on = 'drug_id')
    drugs.tail()

    #print (drugs.groupby('drug_thin_id').size().value_counts())

    atc = d[d.drug_thin_id.apply(lambda x: str(x)[:3] == "ATC")]
    filt = atc.drug_thin_id.apply(lambda x: len(str(x)) <= 13)
    atc_ids = atc[filt][['drug_id', 'drug_thin_id']]
    atc_ids.columns = ['atc_set_id', 'atc_set']
    atc_desc = atc[~filt][['drug_id', 'drug_thin_id']]
    atc_desc.columns = ['atc_set_id', 'atc_desc']
    atc = pd.merge(atc_ids, atc_desc, on = 'atc_set_id')[['atc_set', 'atc_desc']]
    atc.tail()

    atc.groupby('atc_set').size().value_counts()

    drug_2_atc = s[s.set_id.apply(lambda x: str(x)[:3] == 'ATC')][['drug_thin_id', 'set_id']]

    j = pd.merge(drugs,drug_2_atc, on = 'drug_thin_id')
    j = pd.merge(j, atc, left_on = 'set_id', right_on = 'atc_set')
    j = j.groupby('drug_id').first().reset_index()
    j.shape

    j.loc[:, 'nice_name'] = j.drug_desc + ', ' + j.atc_desc
    j.loc[:, 'd'] =  pd.Series('DEF' for i in range(len(j)))

    j[['d', 'drug_id', 'nice_name']].to_csv(os.path.join(dict_path, 'dict.drugs_nice_names'), 
                                            sep = '\t', index = False, header = False)

    fp = open(os.path.join(dict_path, 'dict.drugs_nice_names'), 'r')
    lines = fp.readlines()
    fp.close()
    lines.insert(0, 'SECTION\tDrugs_nice_names')
    fw = open(os.path.join(dict_path, 'dict.drugs_nice_names'), 'w')
    fw.writelines(list(map(lambda line: line.strip() + '\n' ,lines)))
    fw.close()

def create_readcode_dict():
    cfg = Configuration()
    work_dir = fixOS(cfg.work_path)
    raw_path = fixOS(cfg.doc_source_path)
    dict_path = os.path.join(work_dir, 'dicts')
    if not(os.path.exists(dict_path)):
        os.makedirs(dict_path)

    readcodes_name = list(filter(lambda f: f.startswith('Readcodes') ,os.listdir(raw_path)))
    if len(readcodes_name) != 1:
        raise NameError('coudn''t find readcodes_name')
    readcodes_name = readcodes_name[0]
    
    fp = open(os.path.join(raw_path , readcodes_name), 'r')
    txt_in = fp.readlines()
    fp.close()
    txt_in = list(map(lambda ln : '%s %s'%(ln.strip()[:7].replace(' ','_') ,ln.strip().replace(' ','_')) ,txt_in))
    txt_in = '\n'.join(txt_in)
    prep_dict_file2(txt_in ,os.path.join(dict_path,"dict.read_codes"))

def search_fp(ptrn):
    cfg = Configuration()
    raw_path = fixOS(cfg.doc_source_path)
    search_term = ptrn.lower()
    ahd_code_name = list(filter(lambda f: f.lower().startswith(search_term) and f.lower().endswith('.txt') ,os.listdir(raw_path)))
    if len(ahd_code_name) != 1:
        raise NameError('coudn''t find %s'%ptrn)
    ahd_code_name = ahd_code_name[0]
    return ahd_code_name

def create_drugs_dict():
    cfg = Configuration()
    raw_path = fixOS(cfg.doc_source_path)
    old_source_path = fixOS('/server/Data/THIN1601/THINAncil1601/text')
    out_folder = fixOS(cfg.work_path)
    path3 = os.path.join(out_folder, 'dicts')
    if not(os.path.exists(path3)):
        os.makedirs(path3)
    defs = []
    sets = []
    seen = {}
    seen_names = {}
    seen_drug = {}
    for i in range(500):
        defs.append(["DEF",str(i),"Days:"+str(i)])
    start_dc,start_atc, start_bnf = therapy.add_drug_codes(os.path.join(raw_path ,search_fp("DrugCodes")),defs,sets, seen, seen_names, seen_drug)
    start_dc,start_atc, start_bnf = therapy.add_drug_codes(os.path.join(old_source_path ,search_fp("DrugCodes").replace('1701', '1601')),defs,sets, seen, seen_names, seen_drug, start_dc,start_atc, start_bnf)
    start_atc = therapy.add_atc_codes(os.path.join(raw_path ,search_fp("ATCTerms")),defs,sets, seen, seen_names, start_atc)
    therapy.add_atc_codes(os.path.join(old_source_path ,search_fp("ATCTerms").replace('1701', '1601')),defs,sets, seen, seen_names, start_atc)
    start_bnf =  therapy.add_bnf_codes(os.path.join(raw_path ,search_fp("BNFCode")),defs,sets, seen, seen_names, start_bnf)
    therapy.add_bnf_codes(os.path.join(old_source_path ,search_fp("BNFCode").replace('1701', '1601')),defs,sets, seen, seen_names, start_bnf)
    defs.append(["SECTION","Drug"])
    sets.append(["SECTION","Drug"])

    therapy.write_tabbed_list_num(os.path.join(path3,"dict.drugs_defs"),defs)
    therapy.write_tabbed_list(os.path.join(path3,"dict.drugs_sets"),sets)

def get_if_exists(d, key, onlyExists = False):
    if d.get(key) is not None:
        return d[key]
    if len(key) == 0:
        return 'None'
    if onlyExists:
        return 'None'
    return key

def create_hospitalization_dict():
    cfg = Configuration()
    work_path = fixOS(cfg.work_path)
    dict_path = os.path.join(work_path, 'dicts')
    if not(os.path.exists(dict_path)):
        os.makedirs(dict_path)
    primary_convert = { 'X - Ray':'X-Ray', 'General Surgical':'General Surgery', 'Ear| Nose and Throat':'Ear Nose & Throat'}
    sec_convert = {}
    doc_path = fixOS(cfg.doc_source_path)  
    dict_name = list(filter(lambda f: f.startswith('NHSspeciality') ,os.listdir(doc_path)))
    if len(dict_name)!= 1:
        raise NameError('couldn''t find NHSspeciality')
    dict_name = dict_name[0]
    fp = open(os.path.join(doc_path, dict_name), 'r')
    lines = fp.readlines()
    fp.close()
    tokens = list(map(lambda line: [line[:3], get_if_exists(primary_convert, line[3:33].strip()).replace(' ', '_'),
                                    get_if_exists(sec_convert,line[33:].strip()).replace(' ', '_')],lines))
    code2primary = dict()
    code2secondry = dict()
    primary_secondry = dict()
    for code, primary, secondry in tokens:
        if primary_secondry.get(primary) is None:
            primary_secondry[primary] = []
        primary_secondry[primary].append(secondry)

    headerLine = '# This File Was Auto Generated By create_dictionaries.py on %s file @ %s'%(dict_name, datetime.now().strftime('%d-%m-%Y %H:%M:%S'))
    id = 0
    write_lines = [['SECTION', 'Hospitalization,Treatment_Department']]
    write_lines.append([headerLine])
    
    for primary, sec_list in sorted(primary_secondry.items()):
        write_lines.append([])
        write_lines.append(['#Primary: %s'%primary])
        for sec in sec_list:
            comb_name = '%s@%s'%(primary,sec)
            write_lines.append(['DEF', str(id), comb_name])
            id += 1
    write_lines.append([])
    for primary, sec_list in sorted(primary_secondry.items()):
        write_lines.append(['DEF', str(id), primary])
        id += 1
        for sec in sec_list:
            comb_name = '%s@%s'%(primary,sec)
            write_lines.append(['SET', primary, comb_name])
        write_lines.append([])
    fw = open(os.path.join(dict_path, 'dict.hospitalizations') ,'w')
    fw.writelines( list(map(lambda grp: '\t'.join(grp) + '\n' ,write_lines)) )
    fw.close()
    
def fix_duplicate_ids(dict_paths_in_section):
    skip_set = set(['dict.ahdlookups'])
    if len(dict_paths_in_section) == 1 and os.path.basename( dict_paths_in_section[0]) in skip_set:
        print('skip %s - in skip set'%(os.path.basename( dict_paths_in_section[0])))
        return 
    
    print('Running on %s files in same section'%(dict_paths_in_section))
    files_data = dict()
    all_lines = []
    for dict_path in dict_paths_in_section:
        of = open(dict_path,'r')
        lines = list(map(lambda ln: ln.strip(),of.readlines()))
        of.close()
        files_data[dict_path] = lines
        all_lines = all_lines + lines
   
    names = list(map(lambda ln: ln.split('\t')[2] ,filter(lambda ln: ln.startswith('DEF') ,all_lines)))
    cnts = dict()
    for name in names:
        if (cnts.get(name) is None):
            cnts[name] = 0
        cnts[name] = cnts[name] + 1
    #alter duplicate lines:
    
    cnts_pass = dict()
    for file_path, lines in files_data.items():
        altered = []
        altr_cnt = 0
        for line in lines:
            if not(line.startswith('DEF')):
                altered.append(line+ '\n')
                continue
            tokens = line.split('\t')
            if cnts[tokens[2]] > 1:
                if cnts_pass.get(tokens[2]) is None:
                    cnts_pass[tokens[2]] = 0
                cnts_pass[tokens[2]]  = cnts_pass[tokens[2]] + 1
                tokens[2] = tokens[2] + "_#" + str(cnts_pass[tokens[2]])
                altr_cnt = altr_cnt + 1
            altered.append('\t'.join(tokens) + '\n')
        if altr_cnt > 0:
            print('writing file %s with %d alternations'%(file_path, altr_cnt))
            of = open(file_path, 'w')
            of.writelines(altered)
            of.close()

def fix_duplicate_ids_folder():
    cfg = Configuration()
    work_path = fixOS(cfg.work_path)
    dict_path = os.path.join(work_path, 'dicts')
    
    fls = list(filter(lambda f: f.startswith('dict.') ,os.listdir(dict_path)))
    sections = dict()
    section_to_files = dict()
    delimeter = re.compile('[ ,;]')
    for f in fls:
        full_p = os.path.join(dict_path, f)
        fr = open(full_p, 'r')
        lines = list(map(lambda ln: ln.strip(),fr.readlines()))
        fr.close() 
        sname = ['DEFAULT']
        for i in range(min(100, len(lines))):
            if lines[i].upper().startswith('SECTION'):
                #print(f,lines[i].split())
                sname = delimeter.split(lines[i].split()[1])
                break
        target = sname[0]
        for name in sname:
            if sections.get(name) is not None:
                target = sections[name]
                break
        for name in sname:
            sections[name] = target
            if section_to_files.get(target) is None:
                section_to_files[target] = set()
            section_to_files[target].add(full_p)
    
    for section, file_list in section_to_files.items():
        fix_duplicate_ids(list(file_list))

def fix_ahdlookup_dups():
    cfg = Configuration()
    work_path = fixOS(cfg.work_path)
    dict_path = os.path.join(work_path, 'dicts', 'dict.ahdlookups')
    of = open(dict_path,'r')
    lines = list(map(lambda ln: ln.strip(),of.readlines()))
    of.close()

    names = list(map(lambda ln: ln.split('\t')[2] ,filter(lambda ln: ln.startswith('DEF') ,lines)))
    cnts = dict()
    for name in names:
        if (cnts.get(name) is None):
            cnts[name] = 0
        cnts[name] = cnts[name] + 1
    #alter duplicate lines:
    
    cnts_pass = dict()
    
    altered = []
    altr_cnt = 0
    inner_sec = re.compile('\#.* codes:')
    in_section = ''
    for line in lines:
        if not(line.startswith('DEF')):
            if (inner_sec.match(line) is not None):
                in_section = line[1:].split()[0]
            if (len(line) == 0):
                in_section = ''
            altered.append(line+ '\n')
            continue
        tokens = line.split('\t')
        if cnts[tokens[2]] > 1:
            if cnts_pass.get(tokens[2]) is None:
                cnts_pass[tokens[2]] = 0
            cnts_pass[tokens[2]]  = cnts_pass[tokens[2]] + 1
            if (in_section <> ''):
                tokens[2] = in_section + '_' + tokens[2]
            else:
                print('Need change1 %s'%(tokens[2]))
                tokens[2] = tokens[2] + "_#" + str(cnts_pass[tokens[2]])
            if cnts_pass.get(tokens[2]) is None:
                cnts_pass[tokens[2]] = 0
            cnts_pass[tokens[2]]  = cnts_pass[tokens[2]] + 1 #after change
            if cnts_pass[tokens[2]] > 1:
                if in_section <> 'LNG':
                    print('Need change %s'%(tokens[2]))
                tokens[2] = tokens[2] + "_#" + str(cnts_pass[tokens[2]])
            altr_cnt = altr_cnt + 1
        altered.append('\t'.join(tokens) + '\n')
    if altr_cnt > 0:
        print('writing file %s with %d alternations'%(dict_path, altr_cnt))
        of = open(dict_path, 'w')
        of.writelines(altered)
        of.close()
    
if __name__ == "__main__":
    print('Hi')
    #create_pvi_dict()
    #tuples = buildAhdLookup()
    #create_drugs_dict()
    #create_drugs_nice_names()
    #create_readcode_dict()
    #create_hospitalization_dict()